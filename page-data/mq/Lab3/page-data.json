{
    "componentChunkName": "component---src-pages-mq-lab-3-index-md",
    "path": "/mq/Lab3/",
    "result": {"pageContext":{"frontmatter":{"title":"Lab 2 - Deploy a Cloud Native HA persistent IBM MQ Queue Manager on CP4I"},"relativePagePath":"/mq/Lab3/index.md","titleType":"page","MdxNode":{"id":"960f6dfb-369a-5f2a-875d-bf619528f037","children":[],"parent":"bf68bdc8-0c85-50a5-9ee3-9aafdae80267","internal":{"content":"---\ntitle: Lab 2 - Deploy a Cloud Native HA persistent IBM MQ Queue Manager on CP4I\n---\n[Return to main lab page](/mq)\n\nThese instructions will document the process to deploy a NativeHA highly available (HA) persistent IBM MQ on the Cloud Pak for Integration (CP4I) 2022.2.1.\n\n## MQ Native HA Overview\n\nA Native HA configuration provides a highly available queue manager where the recoverable MQ data (for example, the messages)  are replicated across multiple sets of storage, preventing loss from storage failures. The queue manager consists of multiple running instances, one is the leader, the others are ready to quickly take over in the event of a failure, maximizing access to the queue manager and its messages.\n\nA Native HA configuration consists of three Kubernetes pods, each with an instance of the queue manager. One instance is the active queue manager, processing messages and writing to its recovery log. Whenever the recovery log is written, the active queue manager sends the data to the other two instances, known as replicas. Each replica writes to its own recovery log, acknowledges the data, and then updates its own queue data from the replicated recovery log. If the pod running the active queue manager fails, one of the replica instances of the queue manager takes over the active role and has current data to operate with.\n\nA Kubernetes Service is used to route TCP/IP client connections to the current active instance, which is identified as being the only pod which is ready for network traffic. This happens without the need for the client application to be aware of the different instances.\n\nThree pods are used to greatly reduce the possibility of a split-brain situation arising. In a two-pod high availability system split-brain could occur when the connectivity between the two pods breaks. With no connectivity, both pods could run the queue manager at the same time, accumulating different data. When connection is restored, there would be two different versions of the data (a 'split-brain'), and manual intervention is required to decide which data set to keep, and which to discard.\n\nNative HA uses a three pod system with quorum to avoid the split-brain situation. Pods that can communicate with at least one of the other pods form a quorum. A queue manager can only become the active instance on a pod that has quorum. The queue manager cannot become active on a pod that is not connected to at least one other pod, so there can never be two active instances at the same time:\n\n- If a single pod fails, the queue manager on one of the other two pods can take over. If two pods fail, the queue manager cannot become the active instance on the remaining pod because the pod does not have quorum (the remaining pod cannot tell whether the other two pods have failed, or they are still running and it has lost connectivity).\n- If a single pod loses connectivity, the queue manager cannot become active on this pod because the pod does not have quorum. The queue manager on one of the remaining two pods can take over, which do have quorum. If all pods lose connectivity, the queue manager is unable to become active on any of the pods, because none of the pods have quorum.\n- If an active pod fails, and subsequently recovers, it can rejoin the group in a replica role.\n\nThe following figure shows a typical deployment with three instances of a queue manager deployed in three containers.\n\n![](./images/image00.png)\n\n## Deploy the MQ Queue Manager with associated resources\n\nInstead of creating the queues, channels and security settings manually as we did in the previous lab we will provide a configmap that will be used the MQ operator to create/configure all those objects on pod startup.\n\n  The hyperlink to the OpenShift Console for the cluster should be included in your email. Navigate to the OCP console now.\n\n1. Click on the (+) icon at the top right of the Openshift console and paste the contents of the nativehamqsc.yaml file included in this lab. Edit the configmap name prepending your your userid. Also prepend your username in front of both of the occurences of the EXTERNALCHL channel name. In this example we are prepending*cody01*. Use you own user name. After all 3 edits are made click Create at the bottom of the screen.\n\n![](assets/20220710_163056_createconfigmap.png)\n\n  The hyperlink to the Platform Navigator Console for the cluster should be included in your email. Navigate to the Platorm Navigator now.\n\n2. When presented with the \"Log in to IBM Automation\", click *Enterprise LDAP*. Enter the userid and password that you received in your email and click *Login*. Remember to use your credentials, not the ones in the screen shot.\n\n   ![](assets/20220709_221547_0.png)\n\n3. Click on *Integration Instances*.\n\n   ![](assets/20220710_154139_cp4ihome.png)\n\n4. Click *Create an Instance*.\n\n   ![](assets/20220709_222334_instances.png)\n\n5. Select *Messaging* and click *Next*.\n\n   ![](assets/20220709_221720_createMQ.png)\n6. Click select *Quick start* and then click *Next*.\n\n   ![](assets/20220709_221803_createqm.png)\n\n7. Type mq-<<`<your user id>`user user id>>-ha as Name. For example *mq-cody01-ha* and click on the License acceptance checkbox.\n\n   ![](assets/20220709_221911_qmname.png)\n\n8. Scroll down and select NativeHA as type of availability, one of the compatible block storage classes for Default storage class (for example *ibmc-block-gold* in IBM Cloud or *ocs-storagecluster-ceph* for self-managed clsuters) and persistent-clain as Type of volume.\n\n![](assets/20220709_222000_qmavail.png)\n\n9. Click on the*Advanced settings* toggle to show additional properties. Then scroll down to PKI.\n\n![](assets/20220710_155446_advancedpki.png)\n\n10. Type*keystore* as name. Then type*tls.key* in the Items text box and press Enter. Type*tls.crt* and press enter again. Finally select*mq-tls-secret* from the Secret name combo box.\n\nNote: The mq-tls-secret was previously created for your you. It holds the tls private key (tls.key) and public key (tls.crt) needed for TLS channel encryption.\n\n![](assets/20220709_222538_pki.png)\n\n11. Scroll down to the MQSC section. This allows you to provide a mq command script file so that MQ objects are created/configured automatically on pod startup.\n\n12. Type nativehamqsc.mqsc and press enter in the Items text box and then select the name of the configmap you created at the step 1 of this lab in the Name drop down combo, then click*Create*.\n\n![](assets/20220710_163759_mqsc2.png)\n\nThe new Queue Manager instance will be created.\n\nFinally, We need to create an openshift route to allow external application to connect in the cluster using TLS.\n\n13. Click on the (+) icon at the top right of the Openshift console and paste the contents of the mqroute.yaml file included in this lab. Edit the route prepending your your userid to the name and hostname and then click Create.\n\n    Note: The first part of the host URL must match the name of the channel name in the queue manager (which was creted using the configmap) but all lower case. The \"*.chl.mq.ibm.com*\" is always fixed. It does not need to match any real domain name of the cluster. This setting is needed so that by using SNI over TLS the incomming requests can be routed not only to the correct port but also to the correct channel within the MQ pod.\n\n![](assets/20220710_170056_mqroute3.png)\n\n14. Click in the  Workloads->Pods menu item in the OCP console and then filter the pods by typing your username.\n\n![](assets/20220709_223045_viewqms.png)\n\nAs expected you will see three pods for the nativeha queue manager. One of the pods has 1 of 1 containers running. Two of the pods have 0 of 1 containers running. This is the nature of nativeHA, one pod running the queue manager and data being replicated to the other two pods which are in standby mode.\n\n### Viewing the status of Native HA queue managers\n\nYou can view the status of the Native HA instances by running the dspmq command inside one of the running Pods.\n\nYou can use the dspmq command in one of the running Pods to view the operational status of a queue manager instance. The information returned depends on whether the instance is active or a replica. The information supplied by the active instance is definitive, information from replica nodes might be out of date.\nYou can perform the following actions:\n\n* View whether the queue manager instance on the current node is active or a replica.\n* View the Native HA operational status of the instance on the current node.\n* View the operational status of all three instances in a Native HA configuration.\n\nThe following status fields are used to report Native HA configuration status:\n\n* ROLE\n  Specifies the current role of the instance and is one of Active, Replica, or Unknown.\n* INSTANCE\n  The name provided for this instance of the queue manager when it was created using the -lr option of the crtmqm command.\n* INSYNC\n  Indicates whether the instance is able to take over as the active instance if required.\n* QUORUM\n  Reports the quorum status in the form number_of_instances_in-sync/number_of_instances_configured.\n* REPLADDR\n  The replication address of the queue manager instance.\n* CONNACTV\n  Indicates whether the node is connected to the active instance.\n* BACKLOG\n  Indicates the number of KB that the instance is behind.\n* CONNINST\n  Indicates whether the named instance is connected to this instance.\n* ALTDATE\n  Indicates the date on which this information was last updated (blank if it has never been updated).\n* ALTTIME\n  Indicates the time at which this information was last updated (blank if it has never been updated).\n\n1. Click on any of the 3 pods and and then go to the Terminal tab.\n\n![](assets/20220709_223134_terminal1.png)\n\n2. Type*dspmq* and hit Enter to check the status of the local queue manager . You will see that in its status it'll say either Running or Replica depending on if it is the active queue manager or one of the two replicas of the Native HA cluster.\n\n![](assets/20220709_223158_dspmq1.png)\n\n3. Type* dspmq -o nativeha -m QUICKSTART* to display the complete hativeha status of the local queue manager. It'll show the role of the queue manager, nativeha instance name, if it is in sync with the other two replicas and the number of queue managers in quorum.\n\n![](assets/20220709_223230_dspmq2.png)\n\n4. Type*dspmq -o nativeha -x -m QUICKSTART* to display the complete hativeha status of all 3 queue managers in the nativeha cluster.\n\n![](assets/20220709_223404_dspmq3.png)\n\nWe will not test every possibility, but the following are possible displays to expect. Review the possibilities.\n\n* An active instance of the queue manager named **mq00ha** would report the following status:\n\n  QMNAME(mq05ha)                 STATUS(Running)\n\n  * A replica instance of the queue manager would report the following status:\n\n    QMNAME(mq05ha)                 STATUS(Replica)\n\n    * An inactive instance would report the following status:\n\n      QMNAME(mq05ha)                 STATUS(Ended Immediately)\n      To determine Native HA operational status of the instance in the specified pod:\n\n    We will not test every possibility, but the following are possible displays to expect. Review the possibilities.* The active instance of the queue manager named **mq05ha** might report the following status:\n\n    QMNAME(mq05ha)               ROLE(Active) INSTANCE(inst1) INSYNC(Yes) QUORUM(3/3)\n\n    * A replica instance of the queue manager might report the following status:\n\n      QMNAME(mq05ha)               ROLE(Replica) INSTANCE(inst2) INSYNC(Yes) QUORUM(2/3)\n\n      * An inactive instance of the queue manager might report the following status:\n\n        QMNAME(mq05ha)               ROLE(Unknown) INSTANCE(inst3) INSYNC(no) QUORUM(0/3)\n\n    To determine the Native HA operational status of all the instances in the Native HA configuration:\n\n    We will not test every possibility, but the following are possible displays to expect. Review the possibilities.* If you issue this command on the node running the active instance of queue manager **mq05ha**, you might receive the following status:\n\n    QMNAME(mq05ha)\t\t\tROLE(Active) INSTANCE(inst1) INSYNC(Yes) QUORUM(3/3)\n    INSTANCE(mq05ha-ibm-mq-0) ROLE(Active)  REPLADDR(mq05ha-ibm-mq-0) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n    INSTANCE(mq05ha-ibm-mq-1) ROLE(Replica) REPLADDR(mq05ha-ibm-mq-1) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n    INSTANCE(mq05ha-ibm-mq-2) ROLE(Replica) REPLADDR(mq05ha-ibm-mq-2) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n\n    * If you issue this command on a node running a replica instance of queue manager **mq05ha**, you might receive the following status, which indicates that one of the replicas is lagging behind:\n\n      QMNAME(mq05ha)\t\t\tROLE(Replica) INSTANCE(inst2) INSYNC(Yes) QUORUM(2/3)\n      INSTANCE(mq05ha-ibm-mq-2) ROLE(Replica) REPLADDR(mq05ha-ibm-mq-2) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n      INSTANCE(mq05ha-ibm-mq-0) ROLE(Active)  REPLADDR(mq05ha-ibm-mq-0) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n      INSTANCE(mq05ha-ibm-mq-1) ROLE(Replica) REPLADDR(mq05ha-ibm-mq-1) \t\t\t\t\tCONNACTV(Yes) INSYNC(No)  \t\t\t\t\tBACKLOG(435) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n\n      * If you issue this command on a node running an inactive instance of queue manager **mq05ha**, you might receive the following status:\n\n        QMNAME(mq05ha)\t\t\tROLE(Unknown) INSTANCE(inst3) INSYNC(no) QUORUM(0/3)\n        INSTANCE(mq05ha-ibm-mq-0) ROLE(Unknown) REPLADDR(mq05ha-ibm-mq-0) \t\t\t\t\tCONNACTV(Unknown) \t\t\t\t\t\t\tINSYNC(Unknown) BACKLOG(Unknown) CONNINST(No) ALTDATE() ALTTIME()\n        INSTANCE(mq05ha-ibm-mq-1) ROLE(Unknown) REPLADDR(mq05ha-ibm-mq-1) \t\t\t\t\tCONNACTV(Unknown) \t\t\t\t\t\t\tINSYNC(Unknown) BACKLOG(Unknown) CONNINST(No) ALTDATE() ALTTIME()\n        INSTANCE(mq05ha-ibm-mq-2) ROLE(Unknown) REPLADDR(mq05ha-ibm-mq-2) \t\t\t\t\tCONNACTV(No) \t\t\t\t\t\t\t\tINSYNC(Unknown) BACKLOG(Unknown) CONNINST(No) ALTDATE() ALTTIME()\n\n        * If you issue the command when the instances are still negotiating which is active and which are replicas, you would receive the following status:\n\n          QMNAME(mq05ha)              STATUS(Negotiating)\n\n      If necessary, use these commands while testing the deployment.\n\n    ## Test the deployment\n\n    #NOTE: PRE-REQUISITE\n    Make sure that you have an MQ Client installed in your local machine.\n    They can be downloaded from:\n\n\n    - Windows:\n      https://www.ibm.com/support/fixcentral/swg/doSelectFixes?options.selectedFixes=9.3.0.0-IBM-MQC-Win64&continue=1\n    - Linux:\n      https://www.ibm.com/support/fixcentral/swg/doSelectFixes?options.selectedFixes=9.3.0.0-IBM-MQC-LinuxX64&continue=1  \n    - MacOS\n      https://developer.ibm.com/tutorials/mq-macos-dev/\n\n      Follow any configuration instructions of the mq client after installation.\n\n  1. The 4 clientkey files corresponding to the certificates configured on the MQ Channel will be provided to you. Download them to a folder in your local machine.\n\n  2. Return the Platform Navigator home page and click on on your queue manager name in the Messaging box\n\n     ![](assets/20220710_174446_PNMessaging.png)\n\n  3. Click on *Download Connection File*\n\n     ![](assets/20220710_181545_connectionFile.png)\n\n  4. Select *QUICKSTART* as channel name and your channel name and *ANY_TLS12_OR_HIGHER* as cipher spec.\n\n     ![](assets/20220710_181912_SelectChannel.png)\n\n  5. Click Next twice reviewing the settings without making any further changes.\n\n     ![](assets/20220710_182202_SelectHostname.png)\n\n  6. Click Create. Save the file to a folder in your local machine\n\n     ![](assets/20220710_182314_CreateConnFile.png)\n\n  7. Open a new terminal window and run the following commands replacing \"*path-to-ccdt*\" and *\"path-to-keyfile\"* with the folders where you downloaded the keyfile and connection file:\n\n     ```\n     export MQCCDTURL='/path-to-ccdt/ibm-mqha-ccdt.json'\n     ```\n     ```\n     export MQSSLKEYR='/path-to-keyfile/clientkey'\n     ```\n  8. Run the follwing command in the terminal window:\n\n     ```\n     amqsputc APPQ1 QUICKSTART\n     ```\n     The sample program amsputc will put the messages to queue **APPQ1** which has a default persistence defined as persistent. These messages should still be available after a failover.\n\n  9. Type any message and press Enter. Type another message and press Enter twice to disconnect from the queue manager. Remember the text of the messages you typed.\n\n  10. In the terminal window enter the following commands replacing \"*path-to-ccdt*\" and *\"path-to-keyfile\"* with the folders where you downloaded the keyfile and connection file:\n\n      ```\n      export MQCCDTURL='/path-to-ccdt/ibm-mqha-ccdt.json'\n      ```\n      ```\n      export MQSSLKEYR='/path-to-keyfile/clientkey'\n      ```\n      ```\n      amqsghac APPQ QUICKSTART\n      ```\n      ![](assets/20220710_174848_getmsgs.png)\n\n      The sample program amqsghac starts running and will wait for messages to arrive on queue **APPQ**.\n\n  11. Open another terminal window and type the following command:\n\n      ```\n      amqsphac APPQ QUICKSTART\n      ```\n  12. The sample program amqsphac will connect to MQ and start sending messages incessantly.\n  \n  13. Return to the window where *amqsghac* is running. You should get a list of all the messages that have been previously sent before running the command and the ones that are being sent after.\n\n      ![](assets/20220710_174809_QMputget.png)\n\n  14. Go back to the OCP console and check the status of the pods:\n      ![](assets/20220709_223045_viewqms.png)\n\n  15. Delete the running pod\n\n      ![](assets/20220710_172025_deletepod.png)\n\n  Once the active pod is deleted, the running programs will then reconnect to the other pod for it to take over.\n\n  ![](./images/image29.png)\n\n  16. Return to the browser tab where OCP is open. In your project, click the drop-down for *Workloads* and select *Pods*. Enter your queue manager name in the *Name* field to filter out the rest. You will see the now a different pod is in 1/1 Ready state.\n\n      ![](./images/image31a.png)\n  17. Return the Platform Navigator home page and click on on your queue manager name in the Messaging box\n\n      ![](assets/20220710_174446_PNMessaging.png)\n  18. Click on Manage.\n\n      ![](assets/20220710_174615_QMManage.png)\n  19. Verify that queue **APPQ1** still has the number of messages you put to the queue earlier.\n\n      ![](./images/image56.png)\n  20. Click the hyperlink for the queue to verify that those are the messages you created.\n\n      ![](./images/image57.png)\n\n  ## Congratulations\n\n  You have completed this lab nativeHA for MQ on CP4I.\n","type":"Mdx","contentDigest":"fc02bab2719350323842799d7cda3a68","owner":"gatsby-plugin-mdx","counter":1481},"frontmatter":{"title":"Lab 2 - Deploy a Cloud Native HA persistent IBM MQ Queue Manager on CP4I"},"exports":{},"rawBody":"---\ntitle: Lab 2 - Deploy a Cloud Native HA persistent IBM MQ Queue Manager on CP4I\n---\n[Return to main lab page](/mq)\n\nThese instructions will document the process to deploy a NativeHA highly available (HA) persistent IBM MQ on the Cloud Pak for Integration (CP4I) 2022.2.1.\n\n## MQ Native HA Overview\n\nA Native HA configuration provides a highly available queue manager where the recoverable MQ data (for example, the messages)  are replicated across multiple sets of storage, preventing loss from storage failures. The queue manager consists of multiple running instances, one is the leader, the others are ready to quickly take over in the event of a failure, maximizing access to the queue manager and its messages.\n\nA Native HA configuration consists of three Kubernetes pods, each with an instance of the queue manager. One instance is the active queue manager, processing messages and writing to its recovery log. Whenever the recovery log is written, the active queue manager sends the data to the other two instances, known as replicas. Each replica writes to its own recovery log, acknowledges the data, and then updates its own queue data from the replicated recovery log. If the pod running the active queue manager fails, one of the replica instances of the queue manager takes over the active role and has current data to operate with.\n\nA Kubernetes Service is used to route TCP/IP client connections to the current active instance, which is identified as being the only pod which is ready for network traffic. This happens without the need for the client application to be aware of the different instances.\n\nThree pods are used to greatly reduce the possibility of a split-brain situation arising. In a two-pod high availability system split-brain could occur when the connectivity between the two pods breaks. With no connectivity, both pods could run the queue manager at the same time, accumulating different data. When connection is restored, there would be two different versions of the data (a 'split-brain'), and manual intervention is required to decide which data set to keep, and which to discard.\n\nNative HA uses a three pod system with quorum to avoid the split-brain situation. Pods that can communicate with at least one of the other pods form a quorum. A queue manager can only become the active instance on a pod that has quorum. The queue manager cannot become active on a pod that is not connected to at least one other pod, so there can never be two active instances at the same time:\n\n- If a single pod fails, the queue manager on one of the other two pods can take over. If two pods fail, the queue manager cannot become the active instance on the remaining pod because the pod does not have quorum (the remaining pod cannot tell whether the other two pods have failed, or they are still running and it has lost connectivity).\n- If a single pod loses connectivity, the queue manager cannot become active on this pod because the pod does not have quorum. The queue manager on one of the remaining two pods can take over, which do have quorum. If all pods lose connectivity, the queue manager is unable to become active on any of the pods, because none of the pods have quorum.\n- If an active pod fails, and subsequently recovers, it can rejoin the group in a replica role.\n\nThe following figure shows a typical deployment with three instances of a queue manager deployed in three containers.\n\n![](./images/image00.png)\n\n## Deploy the MQ Queue Manager with associated resources\n\nInstead of creating the queues, channels and security settings manually as we did in the previous lab we will provide a configmap that will be used the MQ operator to create/configure all those objects on pod startup.\n\n  The hyperlink to the OpenShift Console for the cluster should be included in your email. Navigate to the OCP console now.\n\n1. Click on the (+) icon at the top right of the Openshift console and paste the contents of the nativehamqsc.yaml file included in this lab. Edit the configmap name prepending your your userid. Also prepend your username in front of both of the occurences of the EXTERNALCHL channel name. In this example we are prepending*cody01*. Use you own user name. After all 3 edits are made click Create at the bottom of the screen.\n\n![](assets/20220710_163056_createconfigmap.png)\n\n  The hyperlink to the Platform Navigator Console for the cluster should be included in your email. Navigate to the Platorm Navigator now.\n\n2. When presented with the \"Log in to IBM Automation\", click *Enterprise LDAP*. Enter the userid and password that you received in your email and click *Login*. Remember to use your credentials, not the ones in the screen shot.\n\n   ![](assets/20220709_221547_0.png)\n\n3. Click on *Integration Instances*.\n\n   ![](assets/20220710_154139_cp4ihome.png)\n\n4. Click *Create an Instance*.\n\n   ![](assets/20220709_222334_instances.png)\n\n5. Select *Messaging* and click *Next*.\n\n   ![](assets/20220709_221720_createMQ.png)\n6. Click select *Quick start* and then click *Next*.\n\n   ![](assets/20220709_221803_createqm.png)\n\n7. Type mq-<<`<your user id>`user user id>>-ha as Name. For example *mq-cody01-ha* and click on the License acceptance checkbox.\n\n   ![](assets/20220709_221911_qmname.png)\n\n8. Scroll down and select NativeHA as type of availability, one of the compatible block storage classes for Default storage class (for example *ibmc-block-gold* in IBM Cloud or *ocs-storagecluster-ceph* for self-managed clsuters) and persistent-clain as Type of volume.\n\n![](assets/20220709_222000_qmavail.png)\n\n9. Click on the*Advanced settings* toggle to show additional properties. Then scroll down to PKI.\n\n![](assets/20220710_155446_advancedpki.png)\n\n10. Type*keystore* as name. Then type*tls.key* in the Items text box and press Enter. Type*tls.crt* and press enter again. Finally select*mq-tls-secret* from the Secret name combo box.\n\nNote: The mq-tls-secret was previously created for your you. It holds the tls private key (tls.key) and public key (tls.crt) needed for TLS channel encryption.\n\n![](assets/20220709_222538_pki.png)\n\n11. Scroll down to the MQSC section. This allows you to provide a mq command script file so that MQ objects are created/configured automatically on pod startup.\n\n12. Type nativehamqsc.mqsc and press enter in the Items text box and then select the name of the configmap you created at the step 1 of this lab in the Name drop down combo, then click*Create*.\n\n![](assets/20220710_163759_mqsc2.png)\n\nThe new Queue Manager instance will be created.\n\nFinally, We need to create an openshift route to allow external application to connect in the cluster using TLS.\n\n13. Click on the (+) icon at the top right of the Openshift console and paste the contents of the mqroute.yaml file included in this lab. Edit the route prepending your your userid to the name and hostname and then click Create.\n\n    Note: The first part of the host URL must match the name of the channel name in the queue manager (which was creted using the configmap) but all lower case. The \"*.chl.mq.ibm.com*\" is always fixed. It does not need to match any real domain name of the cluster. This setting is needed so that by using SNI over TLS the incomming requests can be routed not only to the correct port but also to the correct channel within the MQ pod.\n\n![](assets/20220710_170056_mqroute3.png)\n\n14. Click in the  Workloads->Pods menu item in the OCP console and then filter the pods by typing your username.\n\n![](assets/20220709_223045_viewqms.png)\n\nAs expected you will see three pods for the nativeha queue manager. One of the pods has 1 of 1 containers running. Two of the pods have 0 of 1 containers running. This is the nature of nativeHA, one pod running the queue manager and data being replicated to the other two pods which are in standby mode.\n\n### Viewing the status of Native HA queue managers\n\nYou can view the status of the Native HA instances by running the dspmq command inside one of the running Pods.\n\nYou can use the dspmq command in one of the running Pods to view the operational status of a queue manager instance. The information returned depends on whether the instance is active or a replica. The information supplied by the active instance is definitive, information from replica nodes might be out of date.\nYou can perform the following actions:\n\n* View whether the queue manager instance on the current node is active or a replica.\n* View the Native HA operational status of the instance on the current node.\n* View the operational status of all three instances in a Native HA configuration.\n\nThe following status fields are used to report Native HA configuration status:\n\n* ROLE\n  Specifies the current role of the instance and is one of Active, Replica, or Unknown.\n* INSTANCE\n  The name provided for this instance of the queue manager when it was created using the -lr option of the crtmqm command.\n* INSYNC\n  Indicates whether the instance is able to take over as the active instance if required.\n* QUORUM\n  Reports the quorum status in the form number_of_instances_in-sync/number_of_instances_configured.\n* REPLADDR\n  The replication address of the queue manager instance.\n* CONNACTV\n  Indicates whether the node is connected to the active instance.\n* BACKLOG\n  Indicates the number of KB that the instance is behind.\n* CONNINST\n  Indicates whether the named instance is connected to this instance.\n* ALTDATE\n  Indicates the date on which this information was last updated (blank if it has never been updated).\n* ALTTIME\n  Indicates the time at which this information was last updated (blank if it has never been updated).\n\n1. Click on any of the 3 pods and and then go to the Terminal tab.\n\n![](assets/20220709_223134_terminal1.png)\n\n2. Type*dspmq* and hit Enter to check the status of the local queue manager . You will see that in its status it'll say either Running or Replica depending on if it is the active queue manager or one of the two replicas of the Native HA cluster.\n\n![](assets/20220709_223158_dspmq1.png)\n\n3. Type* dspmq -o nativeha -m QUICKSTART* to display the complete hativeha status of the local queue manager. It'll show the role of the queue manager, nativeha instance name, if it is in sync with the other two replicas and the number of queue managers in quorum.\n\n![](assets/20220709_223230_dspmq2.png)\n\n4. Type*dspmq -o nativeha -x -m QUICKSTART* to display the complete hativeha status of all 3 queue managers in the nativeha cluster.\n\n![](assets/20220709_223404_dspmq3.png)\n\nWe will not test every possibility, but the following are possible displays to expect. Review the possibilities.\n\n* An active instance of the queue manager named **mq00ha** would report the following status:\n\n  QMNAME(mq05ha)                 STATUS(Running)\n\n  * A replica instance of the queue manager would report the following status:\n\n    QMNAME(mq05ha)                 STATUS(Replica)\n\n    * An inactive instance would report the following status:\n\n      QMNAME(mq05ha)                 STATUS(Ended Immediately)\n      To determine Native HA operational status of the instance in the specified pod:\n\n    We will not test every possibility, but the following are possible displays to expect. Review the possibilities.* The active instance of the queue manager named **mq05ha** might report the following status:\n\n    QMNAME(mq05ha)               ROLE(Active) INSTANCE(inst1) INSYNC(Yes) QUORUM(3/3)\n\n    * A replica instance of the queue manager might report the following status:\n\n      QMNAME(mq05ha)               ROLE(Replica) INSTANCE(inst2) INSYNC(Yes) QUORUM(2/3)\n\n      * An inactive instance of the queue manager might report the following status:\n\n        QMNAME(mq05ha)               ROLE(Unknown) INSTANCE(inst3) INSYNC(no) QUORUM(0/3)\n\n    To determine the Native HA operational status of all the instances in the Native HA configuration:\n\n    We will not test every possibility, but the following are possible displays to expect. Review the possibilities.* If you issue this command on the node running the active instance of queue manager **mq05ha**, you might receive the following status:\n\n    QMNAME(mq05ha)\t\t\tROLE(Active) INSTANCE(inst1) INSYNC(Yes) QUORUM(3/3)\n    INSTANCE(mq05ha-ibm-mq-0) ROLE(Active)  REPLADDR(mq05ha-ibm-mq-0) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n    INSTANCE(mq05ha-ibm-mq-1) ROLE(Replica) REPLADDR(mq05ha-ibm-mq-1) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n    INSTANCE(mq05ha-ibm-mq-2) ROLE(Replica) REPLADDR(mq05ha-ibm-mq-2) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n\n    * If you issue this command on a node running a replica instance of queue manager **mq05ha**, you might receive the following status, which indicates that one of the replicas is lagging behind:\n\n      QMNAME(mq05ha)\t\t\tROLE(Replica) INSTANCE(inst2) INSYNC(Yes) QUORUM(2/3)\n      INSTANCE(mq05ha-ibm-mq-2) ROLE(Replica) REPLADDR(mq05ha-ibm-mq-2) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n      INSTANCE(mq05ha-ibm-mq-0) ROLE(Active)  REPLADDR(mq05ha-ibm-mq-0) \t\t\t\t\tCONNACTV(Yes) INSYNC(Yes) \t\t\t\t\tBACKLOG(0) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n      INSTANCE(mq05ha-ibm-mq-1) ROLE(Replica) REPLADDR(mq05ha-ibm-mq-1) \t\t\t\t\tCONNACTV(Yes) INSYNC(No)  \t\t\t\t\tBACKLOG(435) CONNINST(Yes) ALTDATE(2021-01-12) ALTTIME(12.03.44)\n\n      * If you issue this command on a node running an inactive instance of queue manager **mq05ha**, you might receive the following status:\n\n        QMNAME(mq05ha)\t\t\tROLE(Unknown) INSTANCE(inst3) INSYNC(no) QUORUM(0/3)\n        INSTANCE(mq05ha-ibm-mq-0) ROLE(Unknown) REPLADDR(mq05ha-ibm-mq-0) \t\t\t\t\tCONNACTV(Unknown) \t\t\t\t\t\t\tINSYNC(Unknown) BACKLOG(Unknown) CONNINST(No) ALTDATE() ALTTIME()\n        INSTANCE(mq05ha-ibm-mq-1) ROLE(Unknown) REPLADDR(mq05ha-ibm-mq-1) \t\t\t\t\tCONNACTV(Unknown) \t\t\t\t\t\t\tINSYNC(Unknown) BACKLOG(Unknown) CONNINST(No) ALTDATE() ALTTIME()\n        INSTANCE(mq05ha-ibm-mq-2) ROLE(Unknown) REPLADDR(mq05ha-ibm-mq-2) \t\t\t\t\tCONNACTV(No) \t\t\t\t\t\t\t\tINSYNC(Unknown) BACKLOG(Unknown) CONNINST(No) ALTDATE() ALTTIME()\n\n        * If you issue the command when the instances are still negotiating which is active and which are replicas, you would receive the following status:\n\n          QMNAME(mq05ha)              STATUS(Negotiating)\n\n      If necessary, use these commands while testing the deployment.\n\n    ## Test the deployment\n\n    #NOTE: PRE-REQUISITE\n    Make sure that you have an MQ Client installed in your local machine.\n    They can be downloaded from:\n\n\n    - Windows:\n      https://www.ibm.com/support/fixcentral/swg/doSelectFixes?options.selectedFixes=9.3.0.0-IBM-MQC-Win64&continue=1\n    - Linux:\n      https://www.ibm.com/support/fixcentral/swg/doSelectFixes?options.selectedFixes=9.3.0.0-IBM-MQC-LinuxX64&continue=1  \n    - MacOS\n      https://developer.ibm.com/tutorials/mq-macos-dev/\n\n      Follow any configuration instructions of the mq client after installation.\n\n  1. The 4 clientkey files corresponding to the certificates configured on the MQ Channel will be provided to you. Download them to a folder in your local machine.\n\n  2. Return the Platform Navigator home page and click on on your queue manager name in the Messaging box\n\n     ![](assets/20220710_174446_PNMessaging.png)\n\n  3. Click on *Download Connection File*\n\n     ![](assets/20220710_181545_connectionFile.png)\n\n  4. Select *QUICKSTART* as channel name and your channel name and *ANY_TLS12_OR_HIGHER* as cipher spec.\n\n     ![](assets/20220710_181912_SelectChannel.png)\n\n  5. Click Next twice reviewing the settings without making any further changes.\n\n     ![](assets/20220710_182202_SelectHostname.png)\n\n  6. Click Create. Save the file to a folder in your local machine\n\n     ![](assets/20220710_182314_CreateConnFile.png)\n\n  7. Open a new terminal window and run the following commands replacing \"*path-to-ccdt*\" and *\"path-to-keyfile\"* with the folders where you downloaded the keyfile and connection file:\n\n     ```\n     export MQCCDTURL='/path-to-ccdt/ibm-mqha-ccdt.json'\n     ```\n     ```\n     export MQSSLKEYR='/path-to-keyfile/clientkey'\n     ```\n  8. Run the follwing command in the terminal window:\n\n     ```\n     amqsputc APPQ1 QUICKSTART\n     ```\n     The sample program amsputc will put the messages to queue **APPQ1** which has a default persistence defined as persistent. These messages should still be available after a failover.\n\n  9. Type any message and press Enter. Type another message and press Enter twice to disconnect from the queue manager. Remember the text of the messages you typed.\n\n  10. In the terminal window enter the following commands replacing \"*path-to-ccdt*\" and *\"path-to-keyfile\"* with the folders where you downloaded the keyfile and connection file:\n\n      ```\n      export MQCCDTURL='/path-to-ccdt/ibm-mqha-ccdt.json'\n      ```\n      ```\n      export MQSSLKEYR='/path-to-keyfile/clientkey'\n      ```\n      ```\n      amqsghac APPQ QUICKSTART\n      ```\n      ![](assets/20220710_174848_getmsgs.png)\n\n      The sample program amqsghac starts running and will wait for messages to arrive on queue **APPQ**.\n\n  11. Open another terminal window and type the following command:\n\n      ```\n      amqsphac APPQ QUICKSTART\n      ```\n  12. The sample program amqsphac will connect to MQ and start sending messages incessantly.\n  \n  13. Return to the window where *amqsghac* is running. You should get a list of all the messages that have been previously sent before running the command and the ones that are being sent after.\n\n      ![](assets/20220710_174809_QMputget.png)\n\n  14. Go back to the OCP console and check the status of the pods:\n      ![](assets/20220709_223045_viewqms.png)\n\n  15. Delete the running pod\n\n      ![](assets/20220710_172025_deletepod.png)\n\n  Once the active pod is deleted, the running programs will then reconnect to the other pod for it to take over.\n\n  ![](./images/image29.png)\n\n  16. Return to the browser tab where OCP is open. In your project, click the drop-down for *Workloads* and select *Pods*. Enter your queue manager name in the *Name* field to filter out the rest. You will see the now a different pod is in 1/1 Ready state.\n\n      ![](./images/image31a.png)\n  17. Return the Platform Navigator home page and click on on your queue manager name in the Messaging box\n\n      ![](assets/20220710_174446_PNMessaging.png)\n  18. Click on Manage.\n\n      ![](assets/20220710_174615_QMManage.png)\n  19. Verify that queue **APPQ1** still has the number of messages you put to the queue earlier.\n\n      ![](./images/image56.png)\n  20. Click the hyperlink for the queue to verify that those are the messages you created.\n\n      ![](./images/image57.png)\n\n  ## Congratulations\n\n  You have completed this lab nativeHA for MQ on CP4I.\n","fileAbsolutePath":"/Users/thomas/Documents/IBM/Projects/Internal/techjam/src/pages/mq/Lab3/index.md"}}},
    "staticQueryHashes": ["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}