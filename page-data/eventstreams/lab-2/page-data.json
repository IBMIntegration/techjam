{
    "componentChunkName": "component---src-pages-eventstreams-lab-2-index-md",
    "path": "/eventstreams/lab-2/",
    "result": {"pageContext":{"frontmatter":{"title":"Producing & Consuming Data with Event Streams"},"relativePagePath":"/eventstreams/lab-2/index.md","titleType":"page","MdxNode":{"id":"b207c030-dc16-5c83-b122-6e7c53dc31c4","children":[],"parent":"f86b36f7-a017-5316-9716-8b6e0a020ade","internal":{"content":"---\ntitle: Producing & Consuming Data with Event Streams\n---\n\n# Introduction\nVersion control can be a nightmare for organizations. With Kafka, it’s no different. With stream processing pipelines, there are no files to act as containers for messages with a single format. Let take a look at how Event Streams handles Schema Management with the Schema Registry.\n\n# Lab Objective\nIn this lab, we’ll do the following: \n-\tCreate a topic and attach a schema to it\n-\tCreate a Kafka user with appropriate rights to produce and consume data\n-\tGather information needed to connect to the Kafka / Schema clusters.\n-\tTest producing / consuming data.\n-\tMake changes to the Schema and see the impact to producer/consumer.\n\n# Pre-Requisites\n-  Have setup the client machine properly. \n-  Able to access the Event Streams web interface. \n\n# Understanding Schema Registry\n## What is a Schema Registry?\n\nSchema Registry provides a serving layer for your metadata. It provides a RESTful interface for storing and retrieving your Avro®, JSON Schema, and Protobuf schemas. \n•\tIt stores a versioned history of all schemas based on a specified subject name strategy, provides multiple compatibility settings.\n•\tAllows evolution of schemas according to the configured compatibility settings and expanded support for these schema types. \n•\tProvides serializers that plug into Apache Kafka® clients that handle schema storage and retrieval for Kafka messages that are sent in any of the supported formats.\n\nIn Event Streams, Schemas are stored in internal Kafka topics by the Apicurio Registry, an open-source schema registry. In addition to storing a versioned history of schemas, Apicurio Registry provides an interface for retrieving them. Each Event Streams cluster has its own instance of Apicurio Registry providing schema registry functionality.\n\n![](images/image-1.png)\n\n## How the Schema Registry Works?\n\nNow, let’s take a look at how the Schema Registry works.\n\n1.\tSending applications request schema from the Schema Registry.\n2.\tThe scheme is used to automatically validates and serializes be for the data is sent.\n3.\tData is sent, serializing makes transmission more efficient. \n4.\tThe receiving application receives the serialized data.\n5.\tReceiving application request the schema from the Schema Registry. \n6.\tReceiving application deserializes the same data automatically as it receives the message.\n\n![](images/image-2.png)\n\n# Lab Procedures\n\n## Creating a topic and attaching a schema to it\n\n1. **Open this URL from a browser.**\n   [EventStreams Console](https://cpd-cp4i.techjam-4fb3233b14921b317ff7e3af2a6d8125-0000.us-south.containers.appdomain.cloud/integration/kafka-clusters/cp4i-es/techjam-es/) \n\n   - Click on “Enterprise LDAP”\n   - Login with your studentID and password. That should bring you to the Event Streams Console home page. \n   <br/>\n\n     ![](images/1.png)\n   <br/>\n2. **Create Topic** <br/>\n      \n   ![](images/image-4.png) <br/>\n\n   - Refer to screenshots below as an example.  <br/>  \n      \n   ![](images/2.png)   <br/>  \n   ![](images/3.png)   <br/>  \n   ![](images/4.png)   <br/>  \n   ![](images/5.png)   <br/>  \n\n3. **Next create the schema and attach to the topic.** <br/>\n   Click on the Schema Registry tab in the left. <br/>\n\n   ![](images/6.png) <br/>\n   \n   Click on Add Schema (in the right) <br/>\n   \n   ![](images/7.png) <br/>\n   \n   Click Upload Definition -> Choose customer.avsc located in the Kafka Client unzipped folder. <br/>\n   **`C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\com\\example`** <br/>\n\n   ![](images/8.png) <br/>\n\n   Check the details and make sure the schema is valid.  \n\n   Change the name of the schema. The name of the schema maps the schema to the topic. To attach this schema to your topic, the schema should be named according to the topic: **<topic_name>-value.**\n\n   For example, if your topic is **“jam60-topic1”**, the schema should be named **“jam60-topic1-value”** <br/>\n\n   ![](images/9.png) <br/>\n\n   Click on Add Schema. The schema is now attached to the topic. <br/>\n\n## Creating a Kafka User with appropriate rights\n\n1.\tGo to the Event Streams home page.\n   Select \"Connect to this Cluster\" -> Generate SCRAM Credentials. <br/>\n\n   ![](images/10.png) <br/>\n\n   ![](images/10-2.png) <br/>\n\n   Refer to the screenshot attached as reference. <br/>\n\n   ![](images/11.png)   <br/>  \n   ![](images/12.png)   <br/>  \n   ![](images/13.png)   <br/>  \n   ![](images/14.png)   <br/>  \n   ![](images/15.png)   <br/> \n\n## Gather Connection Details\n\nCreating connection from Consumer / Producer requires some connectivity details. These details can be gathered from the Event Stream’s portal. Connectivity details needed will depend on type of authentication and SASL mechanism used. \n\nFrom the Event Stream home page, click on “Connect to this Cluster”.  Get the following information from the page. Refer to screenshot below on how to get these.  \n   -  Bootstrap URL  \n   -  Truststore Certificate File. Copy the downloaded file to the Kafka Client folder.  \n   -  Truststore Password. (Password will be generated once Download Certificate is clicked).  \n   -  Schema Registry URL  \n\n![](images/16.png)   <br/> \n![](images/17.png)   <br/> \n\n## Test Producer / Consumer\n\n1.\tPrepare the config.properties file located in C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\\nCheck and change the following fields. The fields not mentioned here can be left default.\n\n|  Field                            | Value                 |  \n|-------------------------|-----------------------------------------|\n|enableschemaavro\t        |  True (as we have schema attached to the topic) |\n|bootstrap.servers\t     |  Enter the URL obtained in previous section **e.g. es1-kafka-bootstrap-cp4i.apps.ocp46.tec.uk.ibm.com:443**|\n|sasl.jaas.config\t        |  Paste this string. Replace the Username and Password.  `org.apache.kafka.common.security.scram.ScramLoginModule required username='<SCRAM_USER>' password='<SCRAM_PASSWORD>'`;\n|sasl.mechanism           |  SCRAM-SHA-512  |\n|security.protocol\t|  SASL_SSL|\n|topic\t|  Topic created previously. **E.g. jam60-topic1**|\n|group.id\t| Enter a Consumer Group ID. You can enter a Consumer Group. Remember that it should have a prefix of your studentID. **E.g. jam60-consumer-group-v1**|\n|ssl.truststore.location\t|  Should point to the Truststore certificate downloaded. **Example:  ./es-cert.p12**|\n|ssl.truststore.password\t|  Enter the Truststore password obtained. |\n|schema.registry.url\t   |  Enter the URL obtained in previous section **e.g. https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com**|\n|schema.registry.basic.auth.user.info\t|  <SCRAM_USER>:<SCRAM_PASSEORD>|\n|schema.registry.ssl.truststore.location\t|  Same as ssl.truststore.location|\n|schema.registry.ssl.truststore.password\t|  Same as ssl.truststore.password|\n\t\nThis is how your config.properties should look like after he changes. This is a sample. Do not copy and paste this contents. \n\n```\n## Mandatory Section ##\n# Set to true if avro schema is enabled for the topic\nenableschemaavro = true\n# Set to true if want to enable Intercept Monitoring.\nenableintercept = false\n# Set this to true if mTLS (2-way authentication) is enabled.\nenablemtls = false\n# Confluent Broker related properties\nbootstrap.servers = minimal-prod-kafka-bootstrap-es.mycluster-rajan09-992844b4e64c83c3dbd5e7b5e2da5328-0000.sng01.containers.appdomain.cloud:443\nsasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username='jam60-kafka01' password='Do0vIJuwnANZ';\n# Options are PLAIN, SCRAM, GSSAPI\nsasl.mechanism=SCRAM-SHA-512\n# Options are SSL, PLAINTEXT, SASL_SSL, SASL_PLAINTEXT\nsecurity.protocol=SASL_SSL\ntopic=jam60-topic1\n#topic=UserDatabase\n# Consumer Group ID\ngroup.id = jam60-student-group-v1 \nclient.id = student-client-v1\n#--------------------------------\n## To be filled in if TLS is enabled for the Brokers\n# Options are PKCS12, JKS, PEM. Password not required for PEM.\nssl.truststore.type=PKCS12\nssl.truststore.location=./es-cert.p12\nssl.truststore.password=muuJr3QFiiwa\n#--------------------------------\n## To be filled if mTLS (Mutual TLS) is enabled in Brokers\nssl.keystore.location=/home/rajan/load_security/kafka.client.keystore.jks\nssl.keystore.password=clientpass\nssl.key.password=clientpass\n#-------------------------------\n## To be filled in if Schema is enabled\nschema.registry.url = https://minimal-prod-ibm-es-ac-reg-external-es.mycluster-rajan09-992844b4e64c83c3dbd5e7b5e2da5328-0000.sng01.containers.appdomain.cloud\n# The following parameter MUST be set to false if connecting to EventStreams (APICURIO Schema).\nauto.register.schemas=false\n## To be filled in if Schema Registry requires Authentication (e.g. with RBAC enabled). Otherwise leave it as default.\nbasic.auth.credentials.source = USER_INFO\nschema.registry.basic.auth.user.info = jam60-kafka01:Do0vIJuwnANZ\n#--------------------------------\n## To be filled in if TLS is enabled for Schema Registry\nschema.registry.ssl.truststore.location=./es-cert.p12\nschema.registry.ssl.truststore.password=muuJr3QFiiwa\n#--------------------------------\n## To be filled if Consumer / Producer Intercept should be turned on\nintercept_bootstrapServers = es3minimal-kafka-bootstrap-es3.mycluster-rajan07-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud:443\nintercept_sasljaas = org.apache.kafka.common.security.scram.ScramLoginModule required username='rajan' password='CfKQZG9Cm7g5';\nintercept_security = SASL_SSL\nintercept_saslmechanism = SCRAM-SHA-512\n#--------------------------------\n## To be used when Kerberos Authentication is used\nsasl.kerberos.service.name=kafka\n#--------------------------------\n## Required parameters if Confluent in Confluent Cloud is used\nretries = 2\n```\n\n2. Test Producing Message\n\nGoto this folder in command prompt:\n```\ncd C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\ \njava -jar KafkaClient.jar producer 10 config.properties\n```\n\nCheck if the message is listed in the topic. In the Event Streams portal, go to Topics. Look for the topic that you created. Click on it. Then click on messages.  You should see the messages produced. \n***[\\*\\* The messages content may not be displayed correctly in the portal due to deserialization error. ]***\n\n![](images/18.png)   <br/> \n\n3. Test Consuming Message\n\n`java -jar KafkaClient.jar consumer config.properties`\n\nMessages should be consumed correctly.  Message content should be displayed correctly. \nPress CTRL-C to stop the consumer.\n\n![](images/18-2.png)   <br/>\n\n## Check the Impact of Changing the Schema Registry\n\n1.\tWe will change the schema registry and check what happens when producing / consuming. \nIn the client computer, make a copy of the customer.avsc file (located in C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\com\\example>) and name it customer_v2.avsc. You can do this from Windows Explorer.\n\nEdit the file using Notepad++. Add this line right after country. Change the version.\n`{ \"name\": \"company\", \"type\": \"string\", \"doc\": \"Customer Company\" },`\n\nThe customer_v2.avsc should look like this:\n![](images/18-3.png)   <br/>\n\n2. From the Event Streams portal, Go to Schema Registry -> Click on your Schema. Then, click on “Add New Version”.\n   ![](images/19.png)   <br/>\n   \n3. Click on “Upload Definition” and select the edited avsc file (customer_v2.avsc).\n   \n   You should get a validation failed message.\n\n   ![](images/20.png)   <br/>\n\n4.\tUnderstanding Schema Registry Evolution\n\n   When a schema is created, it has to have a compatibility mode. The most used compatibility modes are:\n      -  ***BACKWARD*** - new schema can be used to read data written with old schema [e.g. consumer uses the new schema and read an older offset data]\n      -  ***FORWARD*** - old schema can still be used (e.g. by consumers) to read data written in new schema\n      -  ***FULL*** - Both forward and backward\n   \n   In Event Streams, the default compatibility mode is **FULL**. \n   In our customer_v2.avsc we have added a new mandatory field. Older consumers may not be aware of this field until they update their code. Hence, our schema is NOT FORWARD compatible and so, it fails validation.\n\n5.\tNow, edit the schema file (customer_v2.avsc) again and add a default value to the newly added line. The line should look like this:\n\n   `{ \"name\": \"company\", \"type\": \"string\", \"default\": \"IBM\", \"doc\": \"Customer Company\" },`\n\n   The customer_v2.avsc should look like this.\n\n   ![](images/21.png)   <br/>\n\n6.\tNow try updating the schema. \n   Validation should pass. \n   Change the version number and click on “Add Schema”.\n\n7. Test producing / consuming data\n\n8.\tGetting details about the schema. \n   The Event Streams schema registry supports a Rest Endpoint that provides details about the schema. \n\n   First make sure you have the Basic Authentication Token created during the process of creating the Kafka SCRAM User. If you missed copying the token, you can generate the token from the SCRAM USERNAME and SCRAM PASSWORD. \n   Open this URL: \n   https://www.base64encode.org/ \n\n   Enter your SCRAM USERNAME and SCRAM PASSWORD separated by a colon.\n   E.g. <SCRAM_USER>:<SCRAM_PASSEORD>\n   Click on Encode and it will generate the Basic Authentication Token. \n\n   Get the default compatibility.\n   ```\n   curl -ki -X GET -H “Accept: application/json” -H “Authorization: Basic <BASIC AUTH TOKEN>” https://<SCHEMA_REGISTRY_URL>/rules/COMPATIBILITY\n   E.g. \n   curl -ki -X GET -H “Accept: application/json” -H “Authorization: Basic <BASIC_AUTH_TOKEN>” https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com/rules/COMPATIBILITY\n\n   The response should be something like:\n   {\"config\":\"FULL\",\"type\":\"COMPATIBILITY\"}\n   ```\n   This shows that the default compatibility is FULL. \n\n   Next get the compatibility of the specific schema that we are using. \n   ```\n   curl -ki -X GET -H “Accept: application/json” -H “Authorization: Basic <BASIC_AUTH_TOKEN>” https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com/artifacts/<YOUR_SCHEMA_NAME>/rules\n   ```\n   This should give you an empty response []  \n   Which basically means – the schema uses the default global setting – which is FULL (as we saw when we tried changing the schema).\n\n\n\n","type":"Mdx","contentDigest":"6dec937036085403ec911e091d8f624e","owner":"gatsby-plugin-mdx","counter":1492},"frontmatter":{"title":"Producing & Consuming Data with Event Streams"},"exports":{},"rawBody":"---\ntitle: Producing & Consuming Data with Event Streams\n---\n\n# Introduction\nVersion control can be a nightmare for organizations. With Kafka, it’s no different. With stream processing pipelines, there are no files to act as containers for messages with a single format. Let take a look at how Event Streams handles Schema Management with the Schema Registry.\n\n# Lab Objective\nIn this lab, we’ll do the following: \n-\tCreate a topic and attach a schema to it\n-\tCreate a Kafka user with appropriate rights to produce and consume data\n-\tGather information needed to connect to the Kafka / Schema clusters.\n-\tTest producing / consuming data.\n-\tMake changes to the Schema and see the impact to producer/consumer.\n\n# Pre-Requisites\n-  Have setup the client machine properly. \n-  Able to access the Event Streams web interface. \n\n# Understanding Schema Registry\n## What is a Schema Registry?\n\nSchema Registry provides a serving layer for your metadata. It provides a RESTful interface for storing and retrieving your Avro®, JSON Schema, and Protobuf schemas. \n•\tIt stores a versioned history of all schemas based on a specified subject name strategy, provides multiple compatibility settings.\n•\tAllows evolution of schemas according to the configured compatibility settings and expanded support for these schema types. \n•\tProvides serializers that plug into Apache Kafka® clients that handle schema storage and retrieval for Kafka messages that are sent in any of the supported formats.\n\nIn Event Streams, Schemas are stored in internal Kafka topics by the Apicurio Registry, an open-source schema registry. In addition to storing a versioned history of schemas, Apicurio Registry provides an interface for retrieving them. Each Event Streams cluster has its own instance of Apicurio Registry providing schema registry functionality.\n\n![](images/image-1.png)\n\n## How the Schema Registry Works?\n\nNow, let’s take a look at how the Schema Registry works.\n\n1.\tSending applications request schema from the Schema Registry.\n2.\tThe scheme is used to automatically validates and serializes be for the data is sent.\n3.\tData is sent, serializing makes transmission more efficient. \n4.\tThe receiving application receives the serialized data.\n5.\tReceiving application request the schema from the Schema Registry. \n6.\tReceiving application deserializes the same data automatically as it receives the message.\n\n![](images/image-2.png)\n\n# Lab Procedures\n\n## Creating a topic and attaching a schema to it\n\n1. **Open this URL from a browser.**\n   [EventStreams Console](https://cpd-cp4i.techjam-4fb3233b14921b317ff7e3af2a6d8125-0000.us-south.containers.appdomain.cloud/integration/kafka-clusters/cp4i-es/techjam-es/) \n\n   - Click on “Enterprise LDAP”\n   - Login with your studentID and password. That should bring you to the Event Streams Console home page. \n   <br/>\n\n     ![](images/1.png)\n   <br/>\n2. **Create Topic** <br/>\n      \n   ![](images/image-4.png) <br/>\n\n   - Refer to screenshots below as an example.  <br/>  \n      \n   ![](images/2.png)   <br/>  \n   ![](images/3.png)   <br/>  \n   ![](images/4.png)   <br/>  \n   ![](images/5.png)   <br/>  \n\n3. **Next create the schema and attach to the topic.** <br/>\n   Click on the Schema Registry tab in the left. <br/>\n\n   ![](images/6.png) <br/>\n   \n   Click on Add Schema (in the right) <br/>\n   \n   ![](images/7.png) <br/>\n   \n   Click Upload Definition -> Choose customer.avsc located in the Kafka Client unzipped folder. <br/>\n   **`C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\com\\example`** <br/>\n\n   ![](images/8.png) <br/>\n\n   Check the details and make sure the schema is valid.  \n\n   Change the name of the schema. The name of the schema maps the schema to the topic. To attach this schema to your topic, the schema should be named according to the topic: **<topic_name>-value.**\n\n   For example, if your topic is **“jam60-topic1”**, the schema should be named **“jam60-topic1-value”** <br/>\n\n   ![](images/9.png) <br/>\n\n   Click on Add Schema. The schema is now attached to the topic. <br/>\n\n## Creating a Kafka User with appropriate rights\n\n1.\tGo to the Event Streams home page.\n   Select \"Connect to this Cluster\" -> Generate SCRAM Credentials. <br/>\n\n   ![](images/10.png) <br/>\n\n   ![](images/10-2.png) <br/>\n\n   Refer to the screenshot attached as reference. <br/>\n\n   ![](images/11.png)   <br/>  \n   ![](images/12.png)   <br/>  \n   ![](images/13.png)   <br/>  \n   ![](images/14.png)   <br/>  \n   ![](images/15.png)   <br/> \n\n## Gather Connection Details\n\nCreating connection from Consumer / Producer requires some connectivity details. These details can be gathered from the Event Stream’s portal. Connectivity details needed will depend on type of authentication and SASL mechanism used. \n\nFrom the Event Stream home page, click on “Connect to this Cluster”.  Get the following information from the page. Refer to screenshot below on how to get these.  \n   -  Bootstrap URL  \n   -  Truststore Certificate File. Copy the downloaded file to the Kafka Client folder.  \n   -  Truststore Password. (Password will be generated once Download Certificate is clicked).  \n   -  Schema Registry URL  \n\n![](images/16.png)   <br/> \n![](images/17.png)   <br/> \n\n## Test Producer / Consumer\n\n1.\tPrepare the config.properties file located in C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\\nCheck and change the following fields. The fields not mentioned here can be left default.\n\n|  Field                            | Value                 |  \n|-------------------------|-----------------------------------------|\n|enableschemaavro\t        |  True (as we have schema attached to the topic) |\n|bootstrap.servers\t     |  Enter the URL obtained in previous section **e.g. es1-kafka-bootstrap-cp4i.apps.ocp46.tec.uk.ibm.com:443**|\n|sasl.jaas.config\t        |  Paste this string. Replace the Username and Password.  `org.apache.kafka.common.security.scram.ScramLoginModule required username='<SCRAM_USER>' password='<SCRAM_PASSWORD>'`;\n|sasl.mechanism           |  SCRAM-SHA-512  |\n|security.protocol\t|  SASL_SSL|\n|topic\t|  Topic created previously. **E.g. jam60-topic1**|\n|group.id\t| Enter a Consumer Group ID. You can enter a Consumer Group. Remember that it should have a prefix of your studentID. **E.g. jam60-consumer-group-v1**|\n|ssl.truststore.location\t|  Should point to the Truststore certificate downloaded. **Example:  ./es-cert.p12**|\n|ssl.truststore.password\t|  Enter the Truststore password obtained. |\n|schema.registry.url\t   |  Enter the URL obtained in previous section **e.g. https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com**|\n|schema.registry.basic.auth.user.info\t|  <SCRAM_USER>:<SCRAM_PASSEORD>|\n|schema.registry.ssl.truststore.location\t|  Same as ssl.truststore.location|\n|schema.registry.ssl.truststore.password\t|  Same as ssl.truststore.password|\n\t\nThis is how your config.properties should look like after he changes. This is a sample. Do not copy and paste this contents. \n\n```\n## Mandatory Section ##\n# Set to true if avro schema is enabled for the topic\nenableschemaavro = true\n# Set to true if want to enable Intercept Monitoring.\nenableintercept = false\n# Set this to true if mTLS (2-way authentication) is enabled.\nenablemtls = false\n# Confluent Broker related properties\nbootstrap.servers = minimal-prod-kafka-bootstrap-es.mycluster-rajan09-992844b4e64c83c3dbd5e7b5e2da5328-0000.sng01.containers.appdomain.cloud:443\nsasl.jaas.config = org.apache.kafka.common.security.scram.ScramLoginModule required username='jam60-kafka01' password='Do0vIJuwnANZ';\n# Options are PLAIN, SCRAM, GSSAPI\nsasl.mechanism=SCRAM-SHA-512\n# Options are SSL, PLAINTEXT, SASL_SSL, SASL_PLAINTEXT\nsecurity.protocol=SASL_SSL\ntopic=jam60-topic1\n#topic=UserDatabase\n# Consumer Group ID\ngroup.id = jam60-student-group-v1 \nclient.id = student-client-v1\n#--------------------------------\n## To be filled in if TLS is enabled for the Brokers\n# Options are PKCS12, JKS, PEM. Password not required for PEM.\nssl.truststore.type=PKCS12\nssl.truststore.location=./es-cert.p12\nssl.truststore.password=muuJr3QFiiwa\n#--------------------------------\n## To be filled if mTLS (Mutual TLS) is enabled in Brokers\nssl.keystore.location=/home/rajan/load_security/kafka.client.keystore.jks\nssl.keystore.password=clientpass\nssl.key.password=clientpass\n#-------------------------------\n## To be filled in if Schema is enabled\nschema.registry.url = https://minimal-prod-ibm-es-ac-reg-external-es.mycluster-rajan09-992844b4e64c83c3dbd5e7b5e2da5328-0000.sng01.containers.appdomain.cloud\n# The following parameter MUST be set to false if connecting to EventStreams (APICURIO Schema).\nauto.register.schemas=false\n## To be filled in if Schema Registry requires Authentication (e.g. with RBAC enabled). Otherwise leave it as default.\nbasic.auth.credentials.source = USER_INFO\nschema.registry.basic.auth.user.info = jam60-kafka01:Do0vIJuwnANZ\n#--------------------------------\n## To be filled in if TLS is enabled for Schema Registry\nschema.registry.ssl.truststore.location=./es-cert.p12\nschema.registry.ssl.truststore.password=muuJr3QFiiwa\n#--------------------------------\n## To be filled if Consumer / Producer Intercept should be turned on\nintercept_bootstrapServers = es3minimal-kafka-bootstrap-es3.mycluster-rajan07-992844b4e64c83c3dbd5e7b5e2da5328-0000.jp-tok.containers.appdomain.cloud:443\nintercept_sasljaas = org.apache.kafka.common.security.scram.ScramLoginModule required username='rajan' password='CfKQZG9Cm7g5';\nintercept_security = SASL_SSL\nintercept_saslmechanism = SCRAM-SHA-512\n#--------------------------------\n## To be used when Kerberos Authentication is used\nsasl.kerberos.service.name=kafka\n#--------------------------------\n## Required parameters if Confluent in Confluent Cloud is used\nretries = 2\n```\n\n2. Test Producing Message\n\nGoto this folder in command prompt:\n```\ncd C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\ \njava -jar KafkaClient.jar producer 10 config.properties\n```\n\nCheck if the message is listed in the topic. In the Event Streams portal, go to Topics. Look for the topic that you created. Click on it. Then click on messages.  You should see the messages produced. \n***[\\*\\* The messages content may not be displayed correctly in the portal due to deserialization error. ]***\n\n![](images/18.png)   <br/> \n\n3. Test Consuming Message\n\n`java -jar KafkaClient.jar consumer config.properties`\n\nMessages should be consumed correctly.  Message content should be displayed correctly. \nPress CTRL-C to stop the consumer.\n\n![](images/18-2.png)   <br/>\n\n## Check the Impact of Changing the Schema Registry\n\n1.\tWe will change the schema registry and check what happens when producing / consuming. \nIn the client computer, make a copy of the customer.avsc file (located in C:\\TechJam\\EventStreams_Lab\\KafkaClient_YYYYMMDD\\com\\example>) and name it customer_v2.avsc. You can do this from Windows Explorer.\n\nEdit the file using Notepad++. Add this line right after country. Change the version.\n`{ \"name\": \"company\", \"type\": \"string\", \"doc\": \"Customer Company\" },`\n\nThe customer_v2.avsc should look like this:\n![](images/18-3.png)   <br/>\n\n2. From the Event Streams portal, Go to Schema Registry -> Click on your Schema. Then, click on “Add New Version”.\n   ![](images/19.png)   <br/>\n   \n3. Click on “Upload Definition” and select the edited avsc file (customer_v2.avsc).\n   \n   You should get a validation failed message.\n\n   ![](images/20.png)   <br/>\n\n4.\tUnderstanding Schema Registry Evolution\n\n   When a schema is created, it has to have a compatibility mode. The most used compatibility modes are:\n      -  ***BACKWARD*** - new schema can be used to read data written with old schema [e.g. consumer uses the new schema and read an older offset data]\n      -  ***FORWARD*** - old schema can still be used (e.g. by consumers) to read data written in new schema\n      -  ***FULL*** - Both forward and backward\n   \n   In Event Streams, the default compatibility mode is **FULL**. \n   In our customer_v2.avsc we have added a new mandatory field. Older consumers may not be aware of this field until they update their code. Hence, our schema is NOT FORWARD compatible and so, it fails validation.\n\n5.\tNow, edit the schema file (customer_v2.avsc) again and add a default value to the newly added line. The line should look like this:\n\n   `{ \"name\": \"company\", \"type\": \"string\", \"default\": \"IBM\", \"doc\": \"Customer Company\" },`\n\n   The customer_v2.avsc should look like this.\n\n   ![](images/21.png)   <br/>\n\n6.\tNow try updating the schema. \n   Validation should pass. \n   Change the version number and click on “Add Schema”.\n\n7. Test producing / consuming data\n\n8.\tGetting details about the schema. \n   The Event Streams schema registry supports a Rest Endpoint that provides details about the schema. \n\n   First make sure you have the Basic Authentication Token created during the process of creating the Kafka SCRAM User. If you missed copying the token, you can generate the token from the SCRAM USERNAME and SCRAM PASSWORD. \n   Open this URL: \n   https://www.base64encode.org/ \n\n   Enter your SCRAM USERNAME and SCRAM PASSWORD separated by a colon.\n   E.g. <SCRAM_USER>:<SCRAM_PASSEORD>\n   Click on Encode and it will generate the Basic Authentication Token. \n\n   Get the default compatibility.\n   ```\n   curl -ki -X GET -H “Accept: application/json” -H “Authorization: Basic <BASIC AUTH TOKEN>” https://<SCHEMA_REGISTRY_URL>/rules/COMPATIBILITY\n   E.g. \n   curl -ki -X GET -H “Accept: application/json” -H “Authorization: Basic <BASIC_AUTH_TOKEN>” https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com/rules/COMPATIBILITY\n\n   The response should be something like:\n   {\"config\":\"FULL\",\"type\":\"COMPATIBILITY\"}\n   ```\n   This shows that the default compatibility is FULL. \n\n   Next get the compatibility of the specific schema that we are using. \n   ```\n   curl -ki -X GET -H “Accept: application/json” -H “Authorization: Basic <BASIC_AUTH_TOKEN>” https://es1-ibm-es-ac-reg-external-cp4i.apps.ocp46.tec.uk.ibm.com/artifacts/<YOUR_SCHEMA_NAME>/rules\n   ```\n   This should give you an empty response []  \n   Which basically means – the schema uses the default global setting – which is FULL (as we saw when we tried changing the schema).\n\n\n\n","fileAbsolutePath":"/Users/thomas/Documents/IBM/Projects/Internal/techjam/src/pages/eventstreams/lab-2/index.md"}}},
    "staticQueryHashes": ["1364590287","137577622","2102389209","2456312558","2746626797","3018647132","3037994772","768070550"]}